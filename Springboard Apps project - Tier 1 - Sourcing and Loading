Welcome to the final project of this Springboard prep course! To give you a taste of your future career, we're going to walk through exactly the kind of notebook that you'd write as a data scientist. In the process, we'll be sure to signpost the general framework for our investigation - the Data Science Pipeline - as well as give reasons for why we're doing what we're doing.

Brief

Did Apple Store apps receive better reviews than Google Play apps?

Stages of the project
Sourcing and loading
Load the two datasets
Pick the columns that we are going to work with
Subsetting the data on this basis
Cleaning, transforming and visualizing
Check the data types and fix them
Add a platform column to both the Apple and the Google dataframes
Changing the column names to prepare for a join
Join the two data sets
Eliminate the NaN values
Filter only those apps that have been reviewed at least once
Summarize the data visually and analytically (by the column platform)
Modelling
Hypothesis formulation
Getting the distribution of the data
Permutation test
Evaluating and concluding
What is our conclusion?
What is our decision?
Other models we could have used.
Importing the libraries
In this case we are going to import pandas, numpy, scipy, random and matplotlib.pyplot

In [ ]:
import _ _ _ as pd
import _ _ _ as np
import _ _ _ as plt
# scipi is a library for statistical tests and visualizations 
from scipy import stats
# random enables us to generate random numbers
import random
Stage 1 - Sourcing and loading data
1a. Source and load the data
Let's download the data from Kaggle. Kaggle is a fantastic resource: a kind of social medium for data scientists, it boasts projects, datasets and news on the freshest libraries and technologies all in one place. The data from the Apple Store can be found here and the data from Google Store can be found here. Download the datasets and save them in your working directory.

In [ ]:
# Now that the files are saved, we want to load them into Python using read_csv and pandas.

# Create a variable called google, and store in it the path of the csv file that contains your google dataset. 
# If your dataset is in the same folder as this notebook, the path will simply be the name of the file. 
google = _ _ _

# Read the csv file into a data frame called Google using the read_csv() pandas method.
Google = pd.read_csv(_ _ _)

# Using the head() pandas method, observe the first three entries.
Google._ _ _
In [ ]:
# Create a variable called apple, and store in it the path of the csv file that contains your apple dataset. 
apple = _ _ _ 

# Read the csv file into a pandas DataFrame object called Apple.
Apple = _ _ _ 

# Observe the first three entries like you did with your other data. 
Apple._ _ _
1b. Pick the columns we'll work with
From the documentation of these datasets, we can infer that the most appropriate columns to answer the brief are:

Google:
Category # Do we need this?
Rating
Reviews
Price (maybe)
Apple:
prime_genre # Do we need this?
user_rating
rating_count_tot
price (maybe)
1c. Subsetting accordingly
Let's select only those columns that we want to work with from both datasets. We'll overwrite the subsets in the original variables.

_ _ _ = Google[['Category', 'Rating', 'Reviews', 'Price']]

# Check the first three entries
_ _ _
Apple = _ _ _ 

# Let's check the first three entries
_ _ _
